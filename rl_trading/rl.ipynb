{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "from utils.multiprocessing import ensemble_create_cases, ensemble_execute, print_args\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## global classes\n",
    "class ReinforcementLearningTradingAgent:\n",
    "    def __init__(self, price_dat, reward_dat, index, state_func=None, action_dict=None, **kwargs):\n",
    "        self.price_dat = price_dat\n",
    "        self.reward_dat = reward_dat\n",
    "        self.index = index\n",
    "\n",
    "        # learning related\n",
    "        self.pol = None\n",
    "        self.Q = {}\n",
    "        self.Sf = state_func\n",
    "        self.A = action_dict\n",
    "\n",
    "        # training proecss related\n",
    "        self.trained = False\n",
    "        self.N = 0\n",
    "\n",
    "        # evaluation related\n",
    "        self._eval = {}\n",
    "        self._summary = None\n",
    "\n",
    "        # misc\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "    @print_args\n",
    "    def delay_init(self, state_func, action_dict):\n",
    "        self.Sf = state_func\n",
    "        self.A = action_dict\n",
    "\n",
    "    # def transform(self, func, replace_raw=False):\n",
    "    #     self.price_dat = func(self.price_dat_raw)\n",
    "    #     if replace_raw:\n",
    "    #         self.price_dat_raw = self.price_dat.copy()\n",
    "    #     return self.price_dat\n",
    "\n",
    "    # @print_args\n",
    "    def extract_policy(self):\n",
    "        self.pol = {q: sorted(self.Q[q].items(), key=lambda x: x[1], reverse=True)[0][0] for q in self.Q}\n",
    "        # return self.pol\n",
    "\n",
    "    # @print_args\n",
    "    def evaluate_policy(self):\n",
    "        # print(f\"{_prev=:.6f} {_curr=:.6f} {_next=:.6f}\")\n",
    "        res = []\n",
    "\n",
    "        for i in range(len(self.index)):\n",
    "            states = self.price_dat[i]\n",
    "            pair_ret = self.reward_dat[i][0]\n",
    "            direction = self.A.get(self.pol.get(tuple([self.Sf(_state) for _state in states]), None), 0)\n",
    "            res.append(direction * pair_ret)\n",
    "\n",
    "        self._eval.update({self.N: res})\n",
    "        # return pd.Series(res, index=self.index[:len(res)])\n",
    "\n",
    "    def get_action(self, state):\n",
    "        sa = self.Q.get(state, {})\n",
    "        ra = [(r, a) for a, r in sa.items()]\n",
    "        if ra:\n",
    "            best_r, best_a = sorted(ra, reverse=True)[0]\n",
    "            return best_r, best_a\n",
    "        else:\n",
    "            return 0, np.random.choice(list(self.A.keys()))\n",
    "\n",
    "    def train(self, n_episodes, alpha_start=0.5, alpha_end=0.05, gamma=0.9, S=-0.005, J=0.9, verbose=False):\n",
    "        alpha_step = (alpha_start - alpha_end) / n_episodes\n",
    "        alpha = alpha_start\n",
    "\n",
    "        for i in range(n_episodes):\n",
    "            # init\n",
    "            states = deepcopy(self.price_dat)\n",
    "            rewards = deepcopy(self.reward_dat)\n",
    "            state, _ = states.pop(0), 0\n",
    "            state = tuple([self.Sf(_state) for _state in state])\n",
    "\n",
    "            self.N += 1\n",
    "            while states:\n",
    "                best_r, best_a = self.get_action(state)\n",
    "                exec_a = np.random.choice(list(self.A.keys()),\n",
    "                                          p=[J if (_a == best_a) else (1 - J) / (len(self.A.keys()) - 1) for _a in\n",
    "                                             self.A.keys()])\n",
    "\n",
    "                # execution\n",
    "                exec_r = self.A[exec_a] * rewards.pop(0)[0] + S * (self.A[exec_a] == 0)\n",
    "\n",
    "                # learning\n",
    "                new_state = tuple([self.Sf(_state) for _state in states.pop(0)])\n",
    "                old_q = self.Q.get(state, {}).get(exec_a, 0)\n",
    "                new_r = self.get_action(new_state)[0]\n",
    "                new_q = old_q + alpha * (exec_r + gamma * new_r - old_q)\n",
    "                self.Q[state] = {**self.Q.get(state, {}), **{exec_a: new_q}}\n",
    "                state = new_state\n",
    "\n",
    "                # self.N += 1\n",
    "                # for _prev, _curr, _next in zip(np.append(1, self.price_dat[:-1]), self.price_dat, np.append(self.price_dat[1:], 1)):\n",
    "                #     # state evaluation\n",
    "                #     state = self.Sf(_prev, _curr, _next)\n",
    "                #     best_r, best_a = self.get_action(state)\n",
    "                #     # print(_next, _curr)\n",
    "                #     exec_a = np.random.choice(list(self.A.keys()), p=[J if (_a == best_a) else (1-J)/(len(self.A.keys())-1) for _a in self.A.keys()])\n",
    "                #\n",
    "                #     # execution\n",
    "                #     exec_r = self.A[exec_a] * ((_next / _curr) - 1) + S * (self.A[exec_a] == 0)\n",
    "                #\n",
    "                #     # learning\n",
    "                #     old_q = self.Q.get(state, {}).get(exec_a, 0)\n",
    "                #     new_r = self.get_action(self.Sf(_curr, _next, None))[0]\n",
    "                #     new_q = old_q + alpha * (exec_r + gamma * new_r - old_q)\n",
    "                #     self.Q[state] = {**self.Q.get(state, {}), **{exec_a: new_q}}\n",
    "\n",
    "                # extract & evaluate policy\n",
    "            self.extract_policy()\n",
    "            self.evaluate_policy()\n",
    "            alpha -= alpha_step\n",
    "            print(f\" EPISODE COMPLETED: {self.N} \".center(30, \"-\"))\n",
    "\n",
    "    def summary(self):\n",
    "        if self._summary is None:\n",
    "            fig, (ax, bx) = plt.subplots(2, 1, figsize=(10, 8))\n",
    "            episodes = pd.DataFrame(self._eval, index=self.index).cumsum()\n",
    "            episodes.plot(ax=ax)\n",
    "            episodes.iloc[-1].plot(ax=bx)\n",
    "            episodes.iloc[-1].rolling(20, min_periods=1).mean().plot()\n",
    "            ax.get_legend().remove()\n",
    "            plt.tight_layout()\n",
    "            self._summary = episodes\n",
    "        return self._summary\n",
    "\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        self.affine1 = nn.Linear(5, 64)\n",
    "        self.dropout = nn.Dropout(p=0.6)\n",
    "        self.affine2 = nn.Linear(64, 3)\n",
    "\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.affine1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(x)\n",
    "        action_scores = self.affine2(x)\n",
    "        return F.softmax(action_scores, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## global vars\n",
    "# data preprocessing\n",
    "csvs_px_dir = os.listdir('data')\n",
    "csvs_px = [pd.read_csv(os.path.join('data', und_name), index_col=0, parse_dates=True) for und_name in csvs_px_dir]\n",
    "\n",
    "for und_name, und_price in zip(csvs_px_dir, csvs_px):\n",
    "    und_price.columns = [und_name.split(\".\")[1]]\n",
    "\n",
    "und_prices = pd.concat(csvs_px, axis=1)\n",
    "hs_prices = und_prices[['HSI', 'HSCE']].pct_change().dropna()\n",
    "cs_prices = und_prices[['CSI500', 'CSI1000I']].pct_change().dropna()\n",
    "us_prices = und_prices[['SPX', 'NDX']].pct_change().dropna()\n",
    "\n",
    "t1 = lambda dat: (1 + dat.values).cumprod(axis=0)\n",
    "t2 = lambda dat: dat[:, 0] / dat[:, 1]\n",
    "t3 = lambda dat: dat / dat.shift(260).fillna(1)\n",
    "\n",
    "# REINFORCE vars\n",
    "# parser = argparse.ArgumentParser(description='PyTorch REINFORCE example')\n",
    "# parser.add_argument('--gamma', type=float, default=0.99, metavar='G')\n",
    "# parser.add_argument('--seed', type=int, default=543, metavar='N')\n",
    "# parser.add_argument('--render', action='store_true')\n",
    "# parser.add_argument('--log-interval', type=int, default=10, metavar='N')\n",
    "# args = parser.parse_args()\n",
    "args = argparse.Namespace(gamma=0.95, seed=543, render=False, log_interval=10)\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "# env = gym.make('CartPole-v1')\n",
    "# env.reset(seed=args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# data plots\n",
    "hs_corr = hs_prices.rolling(260).corr(pairwise=True)\n",
    "hs_corr.unstack()[[(hs_prices.columns)]]\n",
    "hs_corr.plot()\n",
    "\n",
    "cs_corr = cs_prices.rolling(260).corr(pairwise=True)\n",
    "cs_corr.unstack()[[(cs_prices.columns)]]\n",
    "cs_corr.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "policy = Policy()\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-2)\n",
    "eps = np.finfo(np.float32).eps.item()\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "    probs = policy(state)\n",
    "    m = Categorical(probs)\n",
    "    action = m.sample()\n",
    "    policy.saved_log_probs.append(m.log_prob(action))\n",
    "    return action.item()\n",
    "\n",
    "\n",
    "def finish_episode():\n",
    "    R = 0\n",
    "    policy_loss = []\n",
    "    returns = []\n",
    "    for r in policy.rewards[::-1]:\n",
    "        R = r + args.gamma * R\n",
    "        returns.insert(0, R)\n",
    "    returns = torch.tensor(returns)\n",
    "    returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "    for log_prob, R in zip(policy.saved_log_probs, returns):\n",
    "        policy_loss.append(-log_prob * R)\n",
    "    optimizer.zero_grad()\n",
    "    policy_loss = torch.cat(policy_loss).sum()\n",
    "    policy_loss.backward()\n",
    "    optimizer.step()\n",
    "    del policy.rewards[:]\n",
    "    del policy.saved_log_probs[:]\n",
    "\n",
    "\n",
    "def reinforce():\n",
    "    ep_rewards = []\n",
    "    running_rewards = []\n",
    "    running_reward = 0\n",
    "    ts = pd.DataFrame(t2(t1(cs_prices)), index=cs_prices.index)\n",
    "    ts_train = pd.concat([ts.shift(i) for i in range(5, 0, -1)] + [ts.pct_change().shift(-1), ], axis=1).dropna()\n",
    "    reward_mapper = {0: -1, 1: 0, 2: 1}\n",
    "\n",
    "    # for i_episode in count(1):\n",
    "    for i_episode in range(5000):\n",
    "        # shuffle?\n",
    "        ts_train = ts_train.sample(frac=1)\n",
    "\n",
    "        states, rewards = ts_train.iloc[:, :5].values.tolist(), ts_train.iloc[:, 5:].values.tolist()\n",
    "        state, ep_reward = states.pop(0), 0\n",
    "\n",
    "        # for t in range(1, 10000):  # Don't infinite loop while learning\n",
    "        while states:\n",
    "            action = select_action(np.array(state))\n",
    "            # state, reward, done, _ = env.step(action)\n",
    "            state, reward = np.array(states.pop(0)), reward_mapper[action] * rewards.pop(0)[0]\n",
    "            policy.rewards.append(reward)\n",
    "            ep_reward += reward\n",
    "\n",
    "        running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
    "        finish_episode()\n",
    "        ep_rewards.append(ep_reward)\n",
    "        running_rewards.append(running_reward)\n",
    "        if i_episode % args.log_interval == 0:\n",
    "            print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n",
    "                i_episode, ep_reward, running_reward))\n",
    "\n",
    "    return ep_rewards, running_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ep_rewards, running_rewards = reinforce()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def reinforce_backtest():\n",
    "    ts = pd.DataFrame(t2(t1(cs_prices)), index=cs_prices.index)\n",
    "    ts_train = pd.concat([ts.shift(i) for i in range(5, 0, -1)] + [ts.pct_change().shift(-1), ], axis=1).dropna()\n",
    "    reward_mapper = {0: -1, 1: 0, 2: 1}\n",
    "\n",
    "    backtest_rewards = []\n",
    "\n",
    "    states, rewards = ts_train.iloc[:, :5].values.tolist(), ts_train.iloc[:, 5:].values.tolist()\n",
    "    state, ep_reward = states.pop(0), 0\n",
    "\n",
    "    while states:\n",
    "        action = select_action(np.array(state))\n",
    "        state, reward = np.array(states.pop(0)), reward_mapper[action] * rewards.pop(0)[0]\n",
    "        backtest_rewards.append(reward)\n",
    "\n",
    "    return backtest_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Policy Gradient Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(zip(ep_rewards, running_rewards)).plot(title='Policy Gradient REINFORCE rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pd.Series(reinforce_backtest()).cumsum().plot(title='Policy Gradient Latest Ep Backtest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(zip(ep_rewards, running_rewards)).plot(title='Policy Gradient REINFORCE shuffled rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pd.Series(reinforce_backtest()).cumsum().plot(title='Policy Gradient shuffled Latest Ep Backtest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Q Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def q_learning(price_dat, norm=False):\n",
    "    # global opts\n",
    "    A1 = {'B': 1, 'N': 0, 'S': -1}\n",
    "    A2 = {'B': 1, 'LB': 0.5, 'N': 0, 'SB': -0.5, 'S': -1}\n",
    "    RLTA = ReinforcementLearningTradingAgent\n",
    "\n",
    "    # global init\n",
    "    # step 1: this is necessary to create pairwise POD\n",
    "    # step 2: this transform is optional, but recommended\n",
    "    dat = pd.DataFrame(t2(t1(price_dat)), index=price_dat.index)\n",
    "    dat_train = pd.concat([dat.shift(i) for i in range(2, 0, -1)] + [dat.pct_change().shift(-1), ], axis=1).dropna()\n",
    "    if norm:\n",
    "        dat = t3(dat)\n",
    "        states, rewards = t3(dat_train.iloc[:, :2]).values.tolist(), dat_train.iloc[:, 2:].values.tolist()\n",
    "    else:\n",
    "        states, rewards = dat_train.iloc[:, :2].values.tolist(), dat_train.iloc[:, 2:].values.tolist()\n",
    "    rl_agent = RLTA(price_dat=states, reward_dat=rewards, index=dat_train.index)\n",
    "\n",
    "    dat.plot(title=\"Pair premium or discount\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    def S(x):\n",
    "        return np.digitize(x, np.linspace(0.7, 1.3, 31))\n",
    "\n",
    "    def M(rl_agent, state_func, action_dict, **kwargs):\n",
    "        # opt args\n",
    "        J = kwargs.get('J', 0.8)        # explore vs exploit coefficient\n",
    "        S = kwargs.get('S', -0.05)     # inaction penalty - loss of op cost\n",
    "\n",
    "        # copy init\n",
    "        _rl_agent = deepcopy(rl_agent)\n",
    "        _rl_agent.delay_init(state_func, action_dict)\n",
    "\n",
    "        # train & extract policy, call evaluate_policy() to do backtesting\n",
    "        _rl_agent.train(n_episodes=1000, J=J, S=S)\n",
    "        _rl_agent.extract_policy()\n",
    "        return _rl_agent\n",
    "\n",
    "    # do ensemble multi eval\n",
    "    # e_cases: parametric input, otherwise pass as kwargs in ensemble_execute() for func partial\n",
    "    e_cases = ensemble_create_cases({\n",
    "        \"action_dict\": [A1, A2, ],\n",
    "        \"J\": [0.8, 0.6, ]\n",
    "    })\n",
    "    # e_res = ensemble_execute(func=M, rl_agent=rl_agent, state_func=S, cases=e_cases)\n",
    "\n",
    "    # demo: to run a singular case for debugging purpose\n",
    "    res = M(rl_agent=rl_agent, state_func=S, action_dict=A2, J=0.6)\n",
    "    res.summary()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# M(rl_agent=rl_agent, state_func=S, action_dict=A2, J=0.6)\n",
    "res = q_learning(cs_prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# M(rl_agent=rl_agent, state_func=S, action_dict=A2, J=0.6)\n",
    "# normalizer ON\n",
    "res = q_learning(cs_prices, norm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# M(rl_agent=rl_agent, state_func=S, action_dict=A2, J=0.8)\n",
    "# exploit = 0.8, explore = 0.2\n",
    "res = q_learning(cs_prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 2-step lookback\n",
    "q_learning(cs_prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# hang seng no norm\n",
    "q_learning(hs_prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# hs norm\n",
    "q_learning(hs_prices, norm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# us no norm\n",
    "q_learning(us_prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# us norm\n",
    "q_learning(us_prices, norm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
