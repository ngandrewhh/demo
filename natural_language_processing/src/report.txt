1.
The dataset contained thousands of lines of words, of which several points are notable:
1) Why the data is stored in an XML format? On one side it could make the parsing through web (I believe TAs are running a server to train the data) might be easier, storing it in plain text, even removing the spaces between the words would make parsing it by our own easier.
2) All words are basically in lowercase, which prevent redundancy to make the strings lower case. However, some repeated words are observed, might be the reason for the acronym of different cases being repeated on the training data, which could bias the model slightly.
3) Quite a lot of words are not "daily" English words, which could skew the results from generating a most English-like word using the bigram model.
4) A simple word count of unigrams and bigrams can be provided to avoiding words I/O every time.

2.
Dijkstra's algorithm is an greedy algorithm, so it could mean that we are omitting the best results that are not explored yet even when we reached 5 anagrams.
For any incomplete anagram that contains some bigrams with really short distances, that particular aangram can be on the top of the agenda continuously, thus leading to a quick termination of the program.
Dijkstra's in this case can lead to massive reduction in runtime, even for long words e.g. "xayxayhohoho". 

However, when Dijkstra's is applied on words with good balance of vowels and consonants, i.e. characters that can construct very english-like words e.g. "osteoporosis", due to most bigram being similarly good, the runtime of the program would tends to the case of factorial, which the results are not computable (in a short time, but total complexity reduction is still considerable).

Nevertheless, on memory complexity, Dijkstra's beats constructing every anagram possible considerably, with the algorithm consider only fringe nodes and will not reserve memory for nodes that are simply unreachable, and the searching of the "best" anagram is done in systematic manner (compared to a linear search in the permutation space of the input word).

3.
To improve the runtime of algorithm, we can:
1) Limit the agenda to a certain size, such that some lower-ranked fringe states will never be explored;
2) Restrict the words that can be added to the agenda in terms of number of used characters, i.e. if 50 6-character fringe states are already in the agenda, any 6-character fringe states that is added to the agenda after that will simply be discarded.
3) Apply heuristic that section the word in to parts of e.g. 6 chracters, [xylophone] => [oohnle][pxy], running Dijkstra's on the first part, and then run Dijkstra's on the 5 generated anagrams from the first part with the remaining parts as their fringe states. This is more "greedy" in nature, but can deal with much longer words.


